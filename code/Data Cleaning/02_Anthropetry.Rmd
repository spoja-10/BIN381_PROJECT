---
title: "02_Anthropetry"
output:
  word_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

## 1. Load Libraries and Data

```{r load_libraries}
# Data Manipulation
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(purrr)
# Visualization
library(ggplot2)
library(skimr)  # For comprehensive summary
library(janitor)  # for cleaning column names
library(visdat)   # visualize missingness
library(mice)     # for advanced imputation 
```

```{r load_data}
# Load data with correct path from project root
anth_df <- read.csv(here("data", "raw", "anthropometry_national_zaf.csv"))

# Skip first metadata row
anth_df <- anth_df[-1, ]

cat("Initial dataset loaded successfully.\n")
cat("Dimensions:", dim(anth_df), "\n")

```

### Load Data

**Purpose:** Load the raw anthropometry dataset into R.

**What the code does:**\
- Reads the CSV file from the project folder using `here()`.\
- Removes the first row if it contains metadata.\
- Displays initial dataset dimensions.

**Outcome:** Raw dataset `anth_df` is ready for assessment and cleaning.

# 2. Initial Data Assessment -----------------------------------------------

```{r initial_assessment}


# Clean column names
anth_df <- janitor::clean_names(anth_df)
colnames(anth_df)

# Peek at structure and summary
glimpse(anth_df)
skim(anth_df)

# Check missingness visually
vis_miss(anth_df)
```

### Initial Data Assessment

**Purpose:** Understand dataset structure, missingness, and content before cleaning.

**What the code does:**\
- Cleans column names with `clean_names()`.\
- Shows variable types and sample data using `glimpse()`.\
- Generates detailed summary statistics with `skim()`.\
- Visualizes missing values using `vis_miss()`.

**Outcome:** Snapshot of dataset quality, guiding the cleaning steps.

## 3. Data Cleaning Process

### 3.1 Handle Duplicates Systematically

-   Duplicates can distort analysis. We remove exact duplicates to maintain dataset integrity.

```{r handle_duplicates}
# Exact duplicates
cat("Exact duplicates:", sum(duplicated(anth_df)), "\n")

# Keep first occurrence
anth_df <- anth_df %>% distinct()

cat("Dimensions after deduplication:", dim(anth_df), "\n")

```

### Handle Duplicates

**Purpose:** Remove repeated rows to maintain dataset integrity.

**What the code does:**\
- Counts exact duplicates with `duplicated()`.\
- Removes duplicates with `distinct()`.

**Outcome:** Dataset now contains only unique observations.

### 3.2 Convert Data Types

-   Ensures numeric, integer, and logical columns are correctly typed for analysis.This prevents calculation errors and improves data quality.

```{r convert_types}
# Define the columns safely
numeric_cols <- intersect(c("value", "precision", "denominator_weighted", "denominator_unweighted"), colnames(anth_df))
integer_cols <- intersect(c("survey_year", "indicator_order", "characteristic_id", 
                            "characteristic_order", "survey_year_label", "by_variable_id", "region_id"), colnames(anth_df))
logical_cols <- intersect(c("is_total", "is_preferred"), colnames(anth_df))

# Apply conversions only if the columns exist
anth_df <- anth_df %>%
  mutate(
    across(all_of(numeric_cols), as.numeric),
    across(all_of(integer_cols), as.integer),
    across(all_of(logical_cols), ~as.logical(as.integer(.)))
  )

cat("Data types converted successfully.\n")

```

### Convert Data Types

**Purpose:** Ensure numeric, integer, and logical columns are properly typed.

**What the code does:**\
- Converts numeric columns like `value` and `precision`.\
- Converts ID or order columns to integers.\
- Converts flag columns (`is_total`, `is_preferred`) to logical.

**Outcome:** Standardized column types, preventing calculation and modeling errors.

### 3.3 Handle Missing Values

```{r handle_missing}
# 1. Summarize missingness
missing_summary <- data.frame(
  Column = names(anth_df),
  Missing_Count = colSums(is.na(anth_df)),
  Missing_Percent = round(colSums(is.na(anth_df)) / nrow(anth_df) * 100, 2)
) %>% arrange(desc(Missing_Percent))

print(missing_summary)

# 2. Drop columns with >80% missing values
cols_to_drop <- missing_summary %>% filter(Missing_Percent > 80) %>% pull(Column)
if(length(cols_to_drop) > 0){
  anth_df <- anth_df %>% select(-all_of(cols_to_drop))
  cat("Dropped columns with >80% missing:", paste(cols_to_drop, collapse = ", "), "\n")
}

# 3. Impute remaining missing values
# Function to get mode
impute_mode <- function(x) {
  ux <- na.omit(x)
  if(length(ux) == 0) return(x)
  x[is.na(x)] <- names(sort(table(ux), decreasing = TRUE))[1]
  return(x)
}

anth_df <- anth_df %>%
  mutate(across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .))) %>%
  mutate(across(where(is.character), impute_mode))

cat("Remaining NAs after imputation:", sum(is.na(anth_df)), "\n")

# Summarize after handling missing values
summary_stats <- data.frame(
  Column = names(anth_df),
  Type = sapply(anth_df, class),
  Missing_Count = colSums(is.na(anth_df)),
  Missing_Percent = round(colSums(is.na(anth_df)) / nrow(anth_df) * 100, 2)
)

# Print summary
print(summary_stats)

# Optional: visualize missing data (should be none now)
library(visdat)
vis_miss(anth_df) + ggtitle("Missing Values After Imputation")

```

**Purpose:** Address missing data to allow accurate analysis.

**What the code does:**\
- Drops columns with \>40% missing values.\
- Imputes remaining numeric missing values with median.\
- Imputes categorical missing values with mode.

**Outcome:** Dataset is complete, reducing bias in analysis.

### 3.4 Remove redundant columns

-   Metadata columns such as survey type or country identifiers are removed as they do not contribute to analysis.

```{r}

# -----------------------------
# Remove redundant columns
# -----------------------------
# Define columns that are metadata or unnecessary for analysis
redundant_cols <- c("survey_type", "survey_id", "country_name", "iso3")  

# Remove them safely
anth_df <- anth_df %>%
  select(-any_of(redundant_cols))

cat("Redundant columns removed. New dimensions:", dim(anth_df), "\n")

```

**Purpose:** Remove columns with all missing or invalid values.

**What the code does:**\
- Detects columns where all values are NA or NaN.\
- Removes these columns from the dataset.

**Outcome:** Dataset is compact, without empty or unusable variables.

# Handle Outliers

```{r Handle Outliers}
# Detect numeric columns
num_cols <- anth_df %>% select(where(is.numeric))

# Compute IQR bounds
outlier_bounds <- function(x) {
  qnt <- quantile(x, probs=c(0.25, 0.75), na.rm=TRUE)
  iqr <- diff(qnt)
  c(lower=qnt[1]-1.5*iqr, upper=qnt[2]+1.5*iqr)
}

bounds <- map(num_cols, outlier_bounds)

# Winsorize numeric variables
anth_df <- anth_df %>%
  mutate(across(where(is.numeric),
                ~pmin(pmax(., bounds[[cur_column()]]["lower"]),
                      bounds[[cur_column()]]["upper"])))

```

### Handle Outliers

**Purpose:** Reduce the influence of extreme values on analysis.

**What the code does:**\
- Calculates IQR bounds per numeric column.\
- Caps values outside lower/upper bounds (Winsorizing).

**Outcome:** Numeric variables are stabilized, minimizing distortion.

### 3.5 Deal with Noise / Special Values

```{r Noise_Special_Values}


anth_df <- anth_df %>%
  mutate(across(matches("height|weight"),
                ~ifelse(. < 0, median(., na.rm = TRUE), .)))


```

### Handle Noise / Special Values

**Purpose:** Correct logically impossible values.

**What the code does:**\
- Replaces negative height or weight values with column median.

**Outcome:** Dataset values are realistic, ready for analysis.



## 5. Save Cleaned Data

```{r save_data}
write_csv(anth_df, here("data", "processed", "anthropometry_cleaned.csv"))
cat("Cleaned dataset saved to data/processed/anthropometry_cleaned.csv\n")


rm(list=ls())  # clears all objects
```

### Save Cleaned Data

**Purpose:** Persist the cleaned dataset for analysis or sharing.

**What the code does:**\
- Saves as CSV in `data/processed/`.

**Outcome:** Cleaned dataset is stored safely for reproducible analysis.
