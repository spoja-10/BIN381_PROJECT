---
title: "05_DHS_Quickstats"
output:
  word_document: default
  html_document: default
---

# DHS QuickStats (National, South Africa)

## Load Libraries

```{r}
# Data manipulation
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(purrr)

# Visualization and summaries
library(ggplot2)
library(skimr)
library(visdat)
```

## Load the DHS QuickStats dataset

```{r}
dhs_df <- read_csv(here("data", "raw", "dhs-quickstats_national_zaf.csv"))

# Remove first row if it contains metadata
dhs_df <- dhs_df[-1, ]

# Reset row names
rownames(dhs_df) <- NULL

cat("DHS QuickStats dataset loaded successfully.\n")
cat("Dimensions:", dim(dhs_df), "\n")
```


## Initial Data Assessment

```{r}
# Quick glimpse
glimpse(dhs_df)

# Summary of missingness
skim(dhs_df)

# Visualize missing values
vis_miss(dhs_df)

# Standardize column names
dhs_df <- dhs_df %>% janitor::clean_names()
colnames(dhs_df)
```

### Glimpse of the Dataset

A quick look at the first few rows and column types revealed:

- 17 character columns (e.g., ISO3, Indicator, CountryName)

- 10 numeric columns (e.g., Value, Precision)

- 2 logical columns (RegionId, LevelRank)

### Summary of Missing Values

The skimr package summarized missingness:

- Columns such as ByVariableLabel, DenominatorWeighted, CILow, and CIHigh contained missing values.

- These columns would require imputation or handling in later steps.

### Visualization

vis_miss() was used to create a visual map of missing data, which helped identify columns with high missingness at a glance.

Why this step matters: Understanding missing data is crucial for selecting appropriate imputation methods or deciding if columns should be dropped.


## Rename Columns Meaningfully

```{r}

# Replace generic col_1, col_2, ... with actual names
colnames(dhs_df) <- c(
  "iso3", "data_id", "indicator", "value", "precision",
  "dhs_country_code", "country_name", "survey_year", "survey_id",
  "indicator_id", "indicator_order", "indicator_type", "characteristic_id",
  "characteristic_order", "characteristic_category", "characteristic_label",
  "by_variable_id", "by_variable_label", "is_total", "is_preferred",
  "sdrid", "region_id", "survey_year_label", "survey_type",
  "denominator_weighted", "denominator_unweighted", "ci_low", "ci_high",
  "level_rank"
)

cat("Columns renamed to meaningful names.\n")
colnames(dhs_df)


```
Column names were standardized to snake_case using janitor::clean_names().

Additionally, descriptive names were assigned to generic column names (e.g., col_1, col_2) to improve readability.

Example:

- ISO3 → iso3

- DataId → data_id

- Value → value

- Precision → precision

- Benefit: This ensures consistency across analysis scripts and improves interpretability for readers.

## Remove Duplicates

```{r}
# Check for exact duplicates
exact_dups <- sum(duplicated(dhs_df))
cat("Exact duplicate rows:", exact_dups, "\n")

# Remove duplicates, keeping first occurrence
dhs_df <- dhs_df %>%
  distinct(indicator, survey_year, characteristic_id, value, .keep_all = TRUE)

cat("Dimensions after duplicate removal:", dim(dhs_df), "\n")


```

## Remove Redundant & Empty Columns

```{r}
all_na_cols <- dhs_df %>%
  summarise(across(everything(), ~all(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "column", values_to = "all_na") %>%
  filter(all_na) %>%
  pull(column)

if(length(all_na_cols) > 0) {
  dhs_df <- dhs_df %>% select(-all_of(all_na_cols))
  cat("Removed 100% missing columns:\n")
  print(all_na_cols)
} else {
  cat("No columns were 100% missing.\n")
}

```

### Removing Duplicates and Empty Columns

Exact duplicate rows were checked; none were found.

The dataset was then filtered to retain unique combinations of Indicator, Survey Year, Characteristic ID, and Value.

Fully empty columns (region_id and level_rank) were removed.

## Convert Data Types Safely

```{r}
# Numeric columns
numeric_cols <- intersect(c("value", "precision", "ci_low", "ci_high"), colnames(dhs_df))

# Integer columns
integer_cols <- intersect(c("survey_year", "indicator_order", "characteristic_id",
                            "characteristic_order", "survey_year_label", "by_variable_id"), colnames(dhs_df))

# Logical columns
logical_cols <- intersect(c("is_total", "is_preferred"), colnames(dhs_df))

# Apply type conversion
dhs_df <- dhs_df %>%
  mutate(
    across(all_of(numeric_cols), as.numeric),
    across(all_of(integer_cols), as.integer),
    across(all_of(logical_cols), ~as.logical(as.integer(.)))
  )

str(dhs_df)


```
Columns were converted to appropriate types:

- Numeric columns: value, precision, ci_low, ci_high

- Integer columns: survey_year, indicator_order, characteristic_id, etc.

- Logical columns: is_total, is_preferred

- Purpose: Correct data types ensure proper calculations, comparisons, and visualizations

## Handle Missing Values

```{r}


# Define mode function for categorical imputation
impute_mode <- function(x) {
  ux <- na.omit(x)
  if(length(ux) == 0) return(x)
  rep(names(sort(table(ux), decreasing = TRUE))[1], length(x))
}

# Impute missing values
dhs_df <- dhs_df %>%
  mutate(
    # Numeric → median
    across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)),
    
    # Character → mode
    across(where(is.character), ~ifelse(is.na(.), impute_mode(.), .)),
    
    # Logical → FALSE
    across(where(is.logical), ~ifelse(is.na(.), FALSE, .))
  )

# Ensure survey_year_label filled
dhs_df <- dhs_df %>%
  mutate(survey_year_label = ifelse(is.na(survey_year_label), survey_year, survey_year_label))

# Recalculate missing values
missing_summary <- data.frame(
  Column = colnames(dhs_df),
  n_missing = colSums(is.na(dhs_df)),
  total_rows = nrow(dhs_df),
  missing_percent = round(colSums(is.na(dhs_df))/nrow(dhs_df)*100, 2)
)

missing_summary %>% arrange(desc(missing_percent))




```
### Handling Missing Values

Strategy:

1. Numeric columns: Imputed using the median

2. Character columns: Imputed using the mode (most frequent value)

3. Logical columns: Missing values set to FALSE

Special handling: survey_year_label was filled with survey_year where missing.

## Outlier Detection

```{r}

# Identify potential outliers using IQR
numeric_cols <- intersect(c("value", "precision"), colnames(dhs_df))

for(col in numeric_cols) {
  Q1 <- quantile(dhs_df[[col]], 0.25, na.rm = TRUE)
  Q3 <- quantile(dhs_df[[col]], 0.75, na.rm = TRUE)
  IQR_val <- Q3 - Q1
  lower <- Q1 - 1.5*IQR_val
  upper <- Q3 + 1.5*IQR_val
  dhs_df[[paste0(col, "_outlier_flag")]] <- dhs_df[[col]] < lower | dhs_df[[col]] > upper
}

# Winsorize 'value' at 1st and 99th percentile
lower_cap <- quantile(dhs_df$value, 0.01, na.rm = TRUE)
upper_cap <- quantile(dhs_df$value, 0.99, na.rm = TRUE)

dhs_df <- dhs_df %>%
  mutate(value = pmax(pmin(value, upper_cap), lower_cap))

summary(dhs_df$value)


```
- Method: Interquartile Range (IQR) to identify extreme values

- Treatment: Values outside 1.5×IQR were flagged, then Winsorized at the 1st and 99th percentiles.

## Save Cleaned Data

```{r}
write_csv(dhs_df, here("data", "processed", "dhs_quickstats_cleaned.csv"))
cat("Cleaned DHS QuickStats dataset saved.\n")

```
