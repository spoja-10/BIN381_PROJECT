---
title: "1_Access_To_Healthcare"
output: html_document
---

# Data Cleaning: Access to Healthcare

## 1. Load Libraries and Data

```{r load_libraries}
# Data manipulation and cleaning
library(dplyr)
library(tidyr)
library(stringr)
library(readr)
library(here)
# Data visualization
library(ggplot2)
library(visdat) # For missing data visualization
library(skimr)  # For detailed summaries
library(naniar)
library(DT)
library(knitr)
```

```{r load_data}
# Load the raw dataset using a robust path
acc_df <- read.csv(here("data","raw","access-to-health-care_national_zaf.csv"))

# Display initial info
cat("Initial dataset loaded successfully.\n")
cat("Dimensions:", dim(acc_df), "\n")



```

## 2. Initial Data Assessment

### 2.1 First Look

```{r initial_look}
# Display first few rows and structure
head(acc_df, 3) 
```

### 2.2 Remove Metadata Row

```{r remove_metadata}
# Store original dimension for reporting
original_dim <- dim(acc_df)

# Remove first row (metadata)
acc_df <- acc_df[-1, ]

cat("Removed metadata row. New dimensions:", dim(acc_df), "\n")
```

### 2.3 Comprehensive Data Quality Report

```{r quality_report}
skim(acc_df)
```

### 2.4 Visualize Missing Data

```{r missing_visualization}
gg_miss_var(acc_df) + ggtitle("Missing Values per Column")
```

## 3. Data Cleaning Process

### 3.1 Handle Duplicates

```{r handle_duplicates}
# Check for exact duplicates
duplicate_count <- sum(duplicated(acc_df))
cat("Number of exact duplicate rows:", duplicate_count, "\n")
```

### 3.2 Convert Data Types

------------------------------------------------------------------------

### **3.2 Convert Data Types**

**Purpose:** Ensure that each column in the dataset has the **correct data type** so that subsequent analysis and calculations work as expected. Wrong data types (e.g., numbers stored as text) can lead to errors or incorrect results.

**What the code does:**

1.  **Define column groups**:

    -   `numeric_cols` → Columns that store measurements or continuous values (e.g., `Value`, `Precision`).
    -   `integer_cols` → Columns representing whole numbers, IDs, or survey codes.
    -   `id_cols` → Identifier columns stored as text (character) to avoid accidental math operations.
    -   `logical_cols` → Columns representing true/false flags (`IsTotal`, `IsPreferred`).

2.  **Convert columns using `mutate(across(...))`**:

    -   `as.numeric` → Converts to numeric type for calculations.
    -   `as.integer` → Converts to integers.
    -   `as.character` → Converts IDs to text.
    -   `as.logical(as.integer(.))` → Converts numeric 0/1 flags to TRUE/FALSE.

3.  **Trim extra spaces in character columns**:

    -   `str_trim` removes leading/trailing whitespace from text fields, preventing errors in grouping or filtering.

4.  **Preview changes**:

    -   `glimpse(acc_df)` shows updated column types and a quick snapshot of the data.

**Outcome:**

-   All columns now have **consistent and correct data types**.
-   Prevents errors in calculations, filtering, grouping, and plotting.
-   Makes the dataset **analysis-ready**.

**Why it matters for the group:**

-   Clean, standardized data types are **foundational** before handling missing values, outliers, or doing any statistical modeling.

------------------------------------------------------------------------

```{r convert_types}
# Explicitly only select columns that exist
numeric_cols <- c("Value", "Precision", "DenominatorWeighted", "DenominatorUnweighted")
integer_cols <- c("SurveyYear", "IndicatorOrder", "CharacteristicOrder", "SurveyYearLabel", "RegionId")
id_cols <- c("CharacteristicId", "ByVariableId")
logical_cols <- c("IsTotal", "IsPreferred")

acc_df <- acc_df %>%
  mutate(across(any_of(numeric_cols), as.numeric)) %>%
  mutate(across(any_of(integer_cols), as.integer)) %>%
  mutate(across(any_of(id_cols), as.character)) %>%
  mutate(across(any_of(logical_cols), ~as.logical(as.integer(.)))) %>%
  mutate(across(where(is.character), str_trim))

cat("Data types converted successfully.\n")
glimpse(acc_df)
```

### 3.3 Handle Missing Values

------------------------------------------------------------------------

-   **Identify missing data:** Count NAs in all columns to understand the scope of missingness.
-   **Remove unhelpful columns:** Drop columns with more than 80% missing values.
-   **Impute key numeric values:** Fill missing denominators using nearby values within groups (`down` and `up`) and replace remaining NAs with the median.
-   **Remove incomplete rows:** Delete rows missing critical information (`Value` or `Indicator`).

**Result:** The dataset is **more complete, consistent, and ready for analysis**, with minimal risk of missing-value errors affecting results.

------------------------------------------------------------------------

```{r handle_missing}
# Create missing value summary before treatment
missing_before <- colSums(is.na(acc_df))

# Strategy 1: Remove columns with excessive missingness (>80%)
high_missing_cols <- names(missing_before[missing_before > nrow(acc_df) * 0.8])
cat("Columns with >80% missing values:", paste(high_missing_cols, collapse = ", "), "\n")

# Strategy 2: Targeted imputation for specific columns

acc_df <- acc_df %>%
  arrange(SurveyYear, CharacteristicId) %>%
  group_by(Indicator, CharacteristicId) %>%
  fill(DenominatorWeighted, DenominatorUnweighted, .direction = "downup") %>%
  ungroup()

# Strategy 3: Remove rows with missing critical values
acc_df <- acc_df %>%
  filter(!is.na(Value), !is.na(Indicator))

missing_after <- colSums(is.na(acc_df))
cat("Missing values reduced significantly.\n")

acc_df <- acc_df %>%
  mutate(
    DenominatorWeighted = ifelse(is.na(DenominatorWeighted), median(DenominatorWeighted, na.rm = TRUE), DenominatorWeighted),
    DenominatorUnweighted = ifelse(is.na(DenominatorUnweighted), median(DenominatorUnweighted, na.rm = TRUE), DenominatorUnweighted)
  )
```

### 3.4 Remove Redundant Columns

------------------------------------------------------------------------

#### **Remove Redundant Columns – Summary**

-   **Purpose:** Remove columns that are **not useful** for analysis or mostly empty.
-   **What’s removed:** Metadata columns (like `ISO3`, `CountryName`, `SurveyId`) and any column with \>80% missing values.
-   **Result:** Dataset is **cleaner, smaller, and easier to work with**, containing only relevant and populated columns.

------------------------------------------------------------------------

```{r remove_columns}
# Define redundant columns explicitly
redundant_cols <- c("ISO3", "DHS_CountryCode", "CountryName", 
                    "SurveyId", "SurveyType")

# Combine with high-missing columns (if any)
cols_to_drop <- c(redundant_cols, high_missing_cols)

# Remove them safely
acc_df <- acc_df %>%
  select(-any_of(cols_to_drop))

cat("Redundant columns removed. New dimensions:", dim(acc_df), "\n")
```

#### **Why Certain Columns Were Removed**

1.  **Metadata columns (e.g., `ISO3`, `CountryName`, `SurveyId`, `SurveyType`, `DHS_CountryCode`)**

    -   These columns **don’t provide new information** for analysis.
    -   For example, `ISO3` and `CountryName` just identify the country—if all data is already for South Africa, they are redundant.
    -   `SurveyId` and `SurveyType` are identifiers for surveys, not variables we analyze. Keeping them would **clutter the dataset**.

2.  **Columns with \>80% missing values**

    -   Columns that are mostly empty **cannot be reliably analyzed**.
    -   Imputing or filling them would introduce **too much uncertainty**.
    -   Removing them keeps the dataset **focused on meaningful, populated data**.

**Bottom line:** These columns were removed to make the dataset **leaner, more focused, and analysis-ready**, preventing confusion or wasted effort on irrelevant or unreliable data.

------------------------------------------------------------------------

### 3.5 Handle Outliers and Anomalies

```{r handle_outliers}
### 3.5 Handle Outliers and Anomalies

# 1. Automatically detect numeric columns
numeric_cols <- acc_df %>%
  select(where(is.numeric)) %>%
  names()

# 2. Visualize numeric columns individually with boxplots
for(col in numeric_cols) {
  p <- ggplot(acc_df, aes_string(y = col)) +
    geom_boxplot(fill = "lightblue", alpha = 0.7) +
    labs(title = paste("Boxplot of", col), y = col) +
    theme_minimal()
  print(p)
}

# 3. Function to cap outliers using IQR method
cap_outliers <- function(x) {
  if(!is.numeric(x)) return(x)
  
  qnt <- quantile(x, probs = c(0.25, 0.75), na.rm = TRUE)
  iqr <- IQR(x, na.rm = TRUE)
  lower <- qnt[1] - 1.5 * iqr
  upper <- qnt[2] + 1.5 * iqr
  
  capped_points <- sum(x < lower | x > upper, na.rm = TRUE)
  message(paste("Capping", capped_points, "outliers in column", deparse(substitute(x))))
  
  x[x < lower] <- lower
  x[x > upper] <- upper
  return(x)
}

# 4. Apply outlier capping to all numeric columns
acc_df <- acc_df %>%
  mutate(across(all_of(numeric_cols), cap_outliers))

message("Outlier treatment completed for all numeric columns.")

```

---
#### **Handle Outliers – Summary**

* **Purpose:** Detect and control extreme numeric values (outliers) that can distort averages, trends, and model results.
* **Method:**

  1. Visualize numeric columns using boxplots to identify extreme points.
  2. Apply the **IQR (Interquartile Range) method**: values below `Q1 - 1.5*IQR` or above `Q3 + 1.5*IQR` are considered outliers.
  3. Cap these outliers at the calculated boundaries instead of removing them, preserving most of the data.
* **Result:** Numeric variables are more stable, reducing the impact of extreme values while keeping the dataset largely intact and reliable for analysis.
---

### 3.6 Standardize Categorical Variables

#### **Standardize Categorical Variables – Summary**

-   **Purpose:** Ensure all categorical data is **consistent and uniform** to prevent errors during grouping, filtering, or analysis.

-   **Method:**

    1.  **Trim extra spaces** from all text fields to remove accidental leading or trailing whitespace.
    2.  **Standardize capitalization** using title case (e.g., `"yes"` → `"Yes"`).
    3.  **Fix multi-space issues** in key columns like `Indicator` and `CharacteristicLabel` to ensure values like `"Access   to Health"` become `"Access to Health"`.

-   **Result:**

    -   All categorical variables are clean and consistent.
    -   Prevents duplicate or misclassified categories in analyses.
    -   Makes the dataset **ready for accurate visualization, grouping, and statistical analysis**.

------------------------------------------------------------------------

```{r clean_categorical}
# Clean and standardize categorical variables
acc_df <- acc_df %>%
  mutate(
    across(where(is.character), ~str_to_title(str_trim(.))),
    # Standardize specific categorical values
    Indicator = str_replace_all(Indicator, "\\s+", " "),
    CharacteristicLabel = str_replace_all(CharacteristicLabel, "\\s+", " ")
  )

cat("Categorical variables standardized.\n")
```

## 4. Final Data Validation

```{r final_validation}
# Final dimensions
dim_df <- data.frame(
  Rows = nrow(acc_df),
  Columns = ncol(acc_df)
)
kable(dim_df, caption = "Final Dataset Dimensions")

# Missing values summary
missing_summary <- acc_df %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Count")

kable(missing_summary, caption = "Missing Values per Column")

# Data types
data_types <- data.frame(
  Variable = names(acc_df),
  Type = sapply(acc_df, class)
)
kable(data_types, caption = "Data Types of Each Column")

# Sample of final data
head(acc_df, 5) %>% 
  kable(caption = "Sample of Final Cleaned Data")
```

## 5. Save Cleaned Data

```{r save_data}
# Ensure directory exists
if(!dir.exists(here("data", "processed"))) {
  dir.create(here("data", "processed"), recursive = TRUE)
}

# Save cleaned dataset
write_csv(acc_df, here("data", "processed", "healthcare_access_cleaned.csv"))
saveRDS(acc_df, here("data", "processed", "healthcare_access_cleaned.rds"))

cat("Cleaned data saved to data/processed/ directory.\n")
```
