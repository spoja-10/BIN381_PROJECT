---
title: "04_covid_prevention"
output: html_document
---

# Covid 19 Prevention

## 1. Load Libraries & data

```{r }
# Data manipulation
library(dplyr)
library(tidyr)
library(readr)
library(here)
library(purrr)

# Visualization and summaries
library(ggplot2)
library(skimr)
library(visdat)

# Load the COVID-19 prevention dataset
# Load the COVID-19 prevention dataset, skipping first row if it contains metadata
covid_df <- read_csv(here("data", "raw", "covid-19-prevention_national_zaf.csv"))

# Step 2: Remove first row (metadata)
covid_df <- covid_df[-1, ]

# Step 3: Reset row names
rownames(covid_df) <- NULL

# Step 4: Optional: in
cat("COVID-19 prevention dataset loaded successfully.\n")
cat("Dimensions:", dim(covid_df), "\n")

```
## 2. Initial Data Assessment and Column Renaming

- Column names are standardized to lowercase with underscores for readability. The dataset structure, summary statistics, and missingness are explored to identify potential quality issues.

```{r}
# Quick glimpse of dataset
glimpse(covid_df)

# Summary of missingness
skim(covid_df)

# visualize missing values
vis_miss(covid_df)

# Clean column names to lowercase with underscores
covid_df <- covid_df %>% janitor::clean_names()

# Check new names
colnames(covid_df)

```



## Handle Duplicates

```{r}
# Check for exact duplicates
exact_dups <- sum(duplicated(covid_df))
cat("Exact duplicate rows:", exact_dups, "\n")



# Remove all duplicates, keeping first occurrence
covid_df <- covid_df %>%
  distinct(indicator, survey_year, characteristic_id, value, .keep_all = TRUE)

cat("Dimensions after duplicate removal:", dim(covid_df), "\n")


```

## Convert Data Types
- Ensures all numeric, integer, and logical columns have correct types for downstream analysis. Prevents calculation errors and improves consistency.
```{r}
# Convert numeric columns
covid_df <- covid_df %>%
  mutate(
    across(c(value, precision, denominator_weighted, denominator_unweighted, ci_low, ci_high), as.numeric),
    across(c(survey_year, indicator_order, characteristic_id, characteristic_order, survey_year_label, by_variable_id), as.integer),
    across(c(is_total, is_preferred), ~as.logical(as.integer(.)))
  )

```

## Drop Redundant Columns
```{r}

redundant_cols <- c("iso3", "data_id", "dhs_country_code", "country_name", 
                    "survey_id", "indicator_id", "sdrid", "region_id", 
                    "survey_type", "level_rank", "denominator_weighted", "denominator_unweighted")

covid_df <- covid_df %>% select(-any_of(redundant_cols))

# Remove columns that are entirely NA
covid_df <- covid_df %>% select(where(~!all(is.na(.))))

cat("Redundant and empty columns removed.\n")
cat("New dimensions:", dim(covid_df), "\n")

```
## Handle Missing Values
- Numeric columns: filled with the median

- Character columns: filled with the most frequent value

- Logical columns: missing values set to FALSE

- Key metadata (survey_year_label, survey_type) imputed explicitly for clarity
```{r}

covid_df <- covid_df %>%
  select(where(~!all(is.na(.))))


impute_mode <- function(x) {
  ux <- na.omit(x)
  if(length(ux) == 0) return(x)
  rep(names(sort(table(ux), decreasing = TRUE))[1], length(x))
}


covid_df <- covid_df %>%
  mutate(
    # Numeric columns → median
    across(where(is.numeric), ~ifelse(is.na(.), median(., na.rm = TRUE), .)),
    
    # Character columns → mode
    across(where(is.character), ~ifelse(is.na(.), impute_mode(.), .)),
    
    # Logical columns → set missing to FALSE (or TRUE if appropriate)
    across(where(is.logical), ~ifelse(is.na(.), FALSE, .))
  )


missing_summary <- data.frame(
  Column = colnames(covid_df),
  n_missing = colSums(is.na(covid_df)),
  total_rows = nrow(covid_df),
  missing_percent = round(colSums(is.na(covid_df)) / nrow(covid_df) * 100, 2)
)

missing_summary %>% arrange(desc(missing_percent))


```



## Handle Outliers
- Extreme values in value are capped to the IQR boundaries (Winsorization), which reduces their influence while keeping most data intact.
```{r}
# Quick check for extreme values in 'value' and denominators
summary(covid_df$value)
summary(covid_df$denominator_weighted)
summary(covid_df$denominator_unweighted)

# Identify potential outliers using IQR method
outliers <- covid_df %>%
  filter(value > quantile(value, 0.75, na.rm = TRUE) + 1.5 * IQR(value, na.rm = TRUE) |
         value < quantile(value, 0.25, na.rm = TRUE) - 1.5 * IQR(value, na.rm = TRUE))

cat("Potential outlier rows in 'value':", nrow(outliers), "\n")

```

## Final Validation 
```{r}
## Final Dataset Check Before Saving (existing columns only)

# Check dataset dimensions and structure
cat("Final dataset dimensions:", dim(covid_df), "\n")
str(covid_df)

# Identify numeric columns that exist
numeric_cols <- covid_df %>% select(where(is.numeric)) %>% colnames()

# Summarize numeric columns for final inspection
summary(select(covid_df, all_of(numeric_cols)))

# Confirm no remaining missing values in all columns
missing_summary <- covid_df %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(cols = everything(), names_to = "column", values_to = "n_missing") %>%
  mutate(
    total_rows = nrow(covid_df),
    missing_percent = round(n_missing / total_rows * 100, 2)
  )

missing_summary %>% arrange(desc(missing_percent))

```

## Save Dataset
```{r save_cleaned_data}
# Define path to save cleaned CSV
clean_path <- here("data", "processed", "covid_prevention_cleaned_zaf.csv")

# Create folder if it doesn't exist
if(!dir.exists(dirname(clean_path))) dir.create(dirname(clean_path), recursive = TRUE)

# Write cleaned dataset
write_csv(covid_df, clean_path)

cat("Cleaned COVID-19 prevention dataset saved successfully at:\n", clean_path, "\n")

```